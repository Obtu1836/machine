不同的损失函数 在反向传播时梯度不一样的写法

在分类问题中 不管是交叉熵还是softmax函数 都相对于均方误差表现要好
主要是下降速度快
下降快的原因是 交叉熵或者softmax 在求导后 含有dsigmoid()这个函数作为分母
在对sigmoid的偏导连乘时会抵消 